{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6dd7969-bf33-4ee5-8a21-2cb1334babba",
      "metadata": {
        "id": "f6dd7969-bf33-4ee5-8a21-2cb1334babba"
      },
      "source": [
        "This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64b00562"
      },
      "source": [
        "pip install python-frontmatter\n"
      ],
      "id": "64b00562",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "cf58f6ed-aaa5-4dc8-ad06-b5328a3b78b9",
      "metadata": {
        "id": "cf58f6ed-aaa5-4dc8-ad06-b5328a3b78b9"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "e839caa8-7fd6-4f6e-9c57-64fbd6d5b73f",
      "metadata": {
        "id": "e839caa8-7fd6-4f6e-9c57-64fbd6d5b73f"
      },
      "outputs": [],
      "source": [
        "def read_repo_data(repo_owner, repo_name):\n",
        "    \"\"\"\n",
        "    Download and parse all markdown files from a GitHub repository.\n",
        "\n",
        "    Args:\n",
        "        repo_owner: GitHub username or organization\n",
        "        repo_name: Repository name\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file content and metadata\n",
        "    \"\"\"\n",
        "    prefix = 'https://codeload.github.com'\n",
        "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "    resp = requests.get(url)\n",
        "\n",
        "    if resp.status_code != 200:\n",
        "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
        "\n",
        "    repository_data = []\n",
        "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "    for file_info in zf.infolist():\n",
        "        filename = file_info.filename\n",
        "        filename_lower = filename.lower()\n",
        "\n",
        "        if not (filename_lower.endswith('.md')\n",
        "            or filename_lower.endswith('.mdx')):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with zf.open(file_info) as f_in:\n",
        "                content = f_in.read().decode('utf-8', errors='ignore')\n",
        "                post = frontmatter.loads(content)\n",
        "                data = post.to_dict()\n",
        "                data['filename'] = filename\n",
        "                repository_data.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    zf.close()\n",
        "    return repository_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "ff74cd18-4f51-4a23-abbb-790ce0fa86cf",
      "metadata": {
        "id": "ff74cd18-4f51-4a23-abbb-790ce0fa86cf"
      },
      "outputs": [],
      "source": [
        "def sliding_window(seq, size, step):\n",
        "    if size <= 0 or step <= 0:\n",
        "        raise ValueError(\"size and step must be positive\")\n",
        "\n",
        "    n = len(seq)\n",
        "    result = []\n",
        "    for i in range(0, n, step):\n",
        "        chunk = seq[i:i+size]\n",
        "        result.append({'start': i, 'chunk': chunk})\n",
        "        if i + size >= n:\n",
        "            break\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "f6574048-10d4-4d19-b298-a64082cd3371",
      "metadata": {
        "id": "f6574048-10d4-4d19-b298-a64082cd3371"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def split_markdown_by_level(text, level=2):\n",
        "    \"\"\"\n",
        "    Split markdown text by a specific header level.\n",
        "\n",
        "    :param text: Markdown text as a string\n",
        "    :param level: Header level to split on\n",
        "    :return: List of sections as strings\n",
        "    \"\"\"\n",
        "    # This regex matches markdown headers\n",
        "    # For level 2, it matches lines starting with \"## \"\n",
        "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
        "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
        "\n",
        "    # Split and keep the headers\n",
        "    parts = pattern.split(text)\n",
        "\n",
        "    sections = []\n",
        "    for i in range(1, len(parts), 3):\n",
        "        # We step by 3 because regex.split() with\n",
        "        # capturing groups returns:\n",
        "        # [before_match, group1, group2, after_match, ...]\n",
        "        # here group1 is \"## \", group2 is the header text\n",
        "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
        "        header = header.strip()\n",
        "\n",
        "        # Get the content after this header\n",
        "        content = \"\"\n",
        "        if i+2 < len(parts):\n",
        "            content = parts[i+2].strip()\n",
        "\n",
        "        if content:\n",
        "            section = f'{header}\\n\\n{content}'\n",
        "        else:\n",
        "            section = header\n",
        "        sections.append(section)\n",
        "\n",
        "    return sections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "cf60b328-eaee-4d4a-9f0e-20ed6a3f6fb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf60b328-eaee-4d4a-9f0e-20ed6a3f6fb8",
        "outputId": "ed0e74cb-181e-48fc-e09f-c88706322fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': '# FinanceBench: A New Benchmark for Financial Question Answering\\n\\n<p align=\"center\">\\n    <img src=\"fig1.png\" alt=\"drawing\" style=\"width: 400px; display: block; margin: 0 auto; text-align:center;\"/>\\n</p>\\n\\n**Abstract:** \\nFinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). This repository contains an open source sample of 150 annotated examples used in the evaluation and analysis of models assessed in the FinanceBench paper. FinanceBench comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81\\\\% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.\\n\\n[![arXiv](https://img.shields.io/badge/arXiv-2311.11944-<COLOR>.svg)](https://arxiv.org/abs/2311.11944)\\n\\n**Contact:**\\nTo evaluate your models on the full `FinanceBench` dataset, or if you have questions about this work, you can email us at contact@patronus.ai\\n\\n---\\n\\n**Dataset Overview:**\\nThe provided open-source dataset (n=150) consists of two JSONL files (located in `/data/`):\\n\\n\\n**`financebench_open_source.jsonl`**:\\n```text\\n    - financebench_id (int):            Unique identifier of the question\\n    - question (str):                   Question of interest\\n    - answer (str):                     Human-annotated gold answer\\n    - dataset_subset_label (str):       Label to identify in which data subset the question is present (\"OPEN_SOURCE\" or \"CLOSED_SOURCE\")\\n    - evidence (list[dict])             List of EvidenceDict\\'s. \\n    - justification (str)               Human-Annotated justification of the gold answer\\n    - question_type (str)               Type of Question: \\'metrics-generated\\', \\'domain-relevant\\', \\'novel-generated\\' \\n    - question_reasoning (str)          Reasoning Type needed to solve the question\\n    - domain_question_num (str)         ID of domain-relevant questions (`dg01` to `dg25`), \"None\" for \\'metrics-generated\\' and \\'novel-generated\\' questions\\n    - company (str)                     Company of Interest\\n    - doc_name (str)                    Unique Document Identifier. Format: {COMPANY}_{PERIOD}_{TYPE}. Some exceptions have the format {COMPANY}_{PERIOD}_{TYPE}_dated-{DATE}\\n\\n\\n    Each EvidenceDict contains four fields: \\n        - \"evidence_text\" (str):            Extracted evidence text from annotators (sentence, paragraph or page) \\n        - \"evidence_doc_name\" (str):        Unique Document Identifier of the relevant document containing the evidence\\n        - \"evidence_page_num\" (int):        Page number of the evidence text (ZERO-indexed)\\n        - \"evidence_text_full_page\" (str):  Full page extract containing the evidence text\\n ```\\n\\n**`financebench_document_information.jsonl`**:\\n```text\\n    - doc_name (str)            Unique Document Identifier. Format: {COMPANY}_{PERIOD}_{TYPE}\\n    - doc_type (str)            Type of the Document: {\"10K\", \"10Q\", \"8K\", \"EARNINGS\", \"10K_ANNUAL\"}\\n    - doc_period (int)          Period of the relevant financial document\\n    - doc_link (str)            URL of the relevant document\\n    - company (str)             Company \\n    - comany_sector_gics (str)  Company Sector in terms of GICS standard\\n```\\n\\nThe above two files can be loaded and joined using:\\n```python\\ndf_questions = pd.read_json(\"data/financebench_open_source.jsonl\", lines=True)\\ndf_meta = pd.read_json(\"data/financebench_document_information.jsonl\", lines=True)\\ndf_full = pd.merge(df_questions, df_meta, on=\"doc_name\")\\n```\\n\\nThe relevant financial source documents (PDFS) are located in `/pdfs/`\\n\\nIn addition, we provide the human-annotated model completions of the evaluated model configurations in the paper of the open-source dataset in `/results/`\\n\\n---\\n\\n**Citation:** If you use our open-source dataset or refer to our result, please use the following citation:\\n```latex\\n@misc{islam2023financebench,\\n      title={FinanceBench: A New Benchmark for Financial Question Answering}, \\n      author={Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},\\n      year={2023},\\n      eprint={2311.11944},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n```', 'filename': 'financebench-main/README.md'}, {'content': 'Folder where Vector-Databases are saved', 'filename': 'financebench-main/vectorstores/README.md'}]\n"
          ]
        }
      ],
      "source": [
        "from minsearch import Index\n",
        "fin_faq = [\n",
        "    {\"question\": \"What was Apple's revenue in Q4 2022?\", \"answer\": \"Apple reported revenue of $90.15 billion in Q4 2022.\"},\n",
        "    {\"question\": \"Does Tesla disclose revenue from regulatory credits?\", \"answer\": \"Yes, Tesla reported $286 million in revenue from regulatory credits in Q2 2022.\"},\n",
        "    {\"question\": \"What percentage of Microsoft’s revenue comes from cloud services?\", \"answer\": \"In FY2022, Microsoft reported that 51% of its revenue came from cloud-based services.\"},\n",
        "    {\"question\": \"How much cash did Alphabet hold at the end of 2021?\", \"answer\": \"Alphabet reported cash and cash equivalents of $20.9 billion at the end of 2021.\"},\n",
        "    {\"question\": \"Did Amazon’s advertising revenue grow in 2021?\", \"answer\": \"Yes, Amazon’s advertising revenue grew 32% in 2021, reaching $31 billion.\"}\n",
        "]\n",
        "fin_index = Index(\n",
        "    text_fields=[\"question\", \"content\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "fin_index.fit(fin_faq)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fin_index = Index(\n",
        "    text_fields=[\"question\", \"answer\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "fin_index.fit(fin_faq)"
      ],
      "metadata": {
        "id": "cmuQcMTF9bZa"
      },
      "id": "cmuQcMTF9bZa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae474f64"
      },
      "source": [
        "%pip install minsearch"
      ],
      "id": "ae474f64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "548dce64-f79f-4fc5-9a5e-40c42718e31b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "c37d5d241d754c4fa0ae5fe9d77f9735",
            "e3fe8ef401a94a13bb8894b5f33fa92a",
            "b9ca580cfa274544acf829d85fa24b42",
            "a3514fcebbff4dae9731157a6de4891a",
            "70e5301f9f32420eb1d5c5e5b45479f5",
            "50474af7dcc2438a9f48d4fbefb35266",
            "81df698bba3b4d338a651b3defcd93ee",
            "e7b512fd7e5743dda634292ea8721e70",
            "f88d5d99c22d43c2a4561b2492535e91",
            "78a61fedfc98457fbeda697dd2b4dbc2",
            "6971d33e9e8c4542b79b0ad93789e501"
          ]
        },
        "id": "548dce64-f79f-4fc5-9a5e-40c42718e31b",
        "outputId": "d56bf771-8ed3-4b49-f26d-a201794738cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c37d5d241d754c4fa0ae5fe9d77f9735"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping document due to missing 'question' or 'answer' key: financebench-main/README.md\n",
            "Skipping document due to missing 'question' or 'answer' key: financebench-main/vectorstores/README.md\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
        "\n",
        "fin_embeddings = []\n",
        "\n",
        "for d in tqdm(fin_faq):\n",
        "    # Check if 'question' and 'answer' keys exist before accessing them\n",
        "    if 'question' in d and 'answer' in d:\n",
        "        text = d['question'] + ' ' + d['answer']\n",
        "        v = embedding_model.encode(text)\n",
        "        fin_embeddings.append(v)\n",
        "    else:\n",
        "        print(f\"Skipping document due to missing 'question' or 'answer' key: {d.get('filename', 'Unknown file')}\")\n",
        "\n",
        "\n",
        "fin_embeddings = np.array(fin_embeddings)\n",
        "print(fin_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac7d8dc8"
      },
      "source": [
        "display(fin_faq)"
      ],
      "id": "ac7d8dc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic AI(Agentic Library"
      ],
      "metadata": {
        "id": "11wnjLfXoVq1"
      },
      "id": "11wnjLfXoVq1"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Any\n",
        "\n",
        "def text_search(query: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Perform a text-based search on the FAQ index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
        "    \"\"\"\n",
        "    return fin_index.search(query, num_results=2)\n"
      ],
      "metadata": {
        "id": "loddS30CoUI7"
      },
      "id": "loddS30CoUI7",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a  course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gs-qXwjmadK5"
      },
      "id": "gs-qXwjmadK5",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "agent = Agent(\n",
        "     name=\"faq_agent\",\n",
        "     instructions=system_prompt,\n",
        "     tools=[text_search],\n",
        "     model='gpt-4o-mini'\n",
        " )"
      ],
      "metadata": {
        "id": "NBZXbnh7pKiu"
      },
      "id": "NBZXbnh7pKiu",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was Apple's revenue in Q4 2022\"\n",
        "\n"
      ],
      "metadata": {
        "id": "33VyQmLPDOmO"
      },
      "id": "33VyQmLPDOmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n"
      ],
      "metadata": {
        "id": "-0ajHgBzDVNH"
      },
      "id": "-0ajHgBzDVNH",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajQWBTJ65bcb"
      },
      "id": "ajQWBTJ65bcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run(user_prompt=question)"
      ],
      "metadata": {
        "id": "u7miq-nl5Zsa"
      },
      "id": "u7miq-nl5Zsa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.new_messages()\n"
      ],
      "metadata": {
        "id": "s_QXr2y7anvh"
      },
      "id": "s_QXr2y7anvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae25f2be"
      },
      "source": [
        "%pip install pydantic-ai"
      ],
      "id": "ae25f2be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Agents"
      ],
      "metadata": {
        "id": "txQM-X9qcnrg"
      },
      "id": "txQM-X9qcnrg"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
        "\n",
        "\n",
        "def log_entry(agent, messages, source=\"user\"):\n",
        "    tools = []\n",
        "\n",
        "    for ts in agent.toolsets:\n",
        "        tools.extend(ts.tools.keys())\n",
        "\n",
        "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
        "\n",
        "    return {\n",
        "        \"agent_name\": agent.name,\n",
        "        \"system_prompt\": agent._instructions,\n",
        "        \"provider\": agent.model.system,\n",
        "        \"model\": agent.model.model_name,\n",
        "        \"tools\": tools,\n",
        "        \"messages\": dict_messages,\n",
        "        \"source\": source\n",
        "    }"
      ],
      "metadata": {
        "id": "DUosv6gucr50"
      },
      "id": "DUosv6gucr50",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Log"
      ],
      "metadata": {
        "id": "jp8BEZBldlvZ"
      },
      "id": "jp8BEZBldlvZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import secrets\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "LOG_DIR = Path('logs')\n",
        "LOG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def serializer(obj):\n",
        "    if isinstance(obj, datetime):\n",
        "        return obj.isoformat()\n",
        "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
        "\n",
        "\n",
        "def log_interaction_to_file(agent, messages, source='user'):\n",
        "    entry = log_entry(agent, messages, source)\n",
        "\n",
        "    ts = entry['messages'][-1]['timestamp']\n",
        "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    rand_hex = secrets.token_hex(3)\n",
        "\n",
        "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
        "    filepath = LOG_DIR / filename\n",
        "\n",
        "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
        "        json.dump(entry, f_out, indent=2, default=serializer)\n",
        "\n",
        "    return filepath"
      ],
      "metadata": {
        "id": "n4Vh_5fsdkep"
      },
      "id": "n4Vh_5fsdkep",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input()\n",
        "result = await agent.run(user_prompt=question)\n",
        "print(result.output)\n",
        "log_interaction_to_file(agent, result.new_messages())"
      ],
      "metadata": {
        "id": "igjntjNvd7_M"
      },
      "id": "igjntjNvd7_M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was Apple's revenue in Q4 2022\"\n",
        "\n"
      ],
      "metadata": {
        "id": "xUtugZv250X0"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xUtugZv250X0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding References"
      ],
      "metadata": {
        "id": "Yb63jO2aePKz"
      },
      "id": "Yb63jO2aePKz"
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "\n",
        "Always include references by citing the filename of the source material you used.\n",
        "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
        "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
        "\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Create another version of agent, let's call it faq_agent_v2\n",
        "agent = Agent(\n",
        "    name=\"faq_agent_v2\",\n",
        "    instructions=system_prompt,\n",
        "    tools=[text_search],\n",
        "    model='gpt-4o-mini'\n",
        ")\n"
      ],
      "metadata": {
        "id": "5gfpPbo9eOD7"
      },
      "id": "5gfpPbo9eOD7",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM as a Judge"
      ],
      "metadata": {
        "id": "q-4gamcee-JD"
      },
      "id": "q-4gamcee-JD"
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_prompt = \"\"\"\n",
        "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
        "We also include the entire log (<LOG>) for analysis.\n",
        "\n",
        "For each item, check if the condition is met.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
        "- instructions_avoid: The agent avoided doing things it was told not to do\n",
        "- answer_relevant: The response directly addresses the user's question\n",
        "- answer_clear: The answer is clear and correct\n",
        "- answer_citations: The response includes proper citations or sources when required\n",
        "- completeness: The response is complete and covers all key aspects of the request\n",
        "- tool_call_search: Is the search tool invoked?\n",
        "\n",
        "Output true/false for each check and provide a short explanation for your judgment.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "laHceNCSe8Ah"
      },
      "id": "laHceNCSe8Ah",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM will produce output that matches this schema exactly.\n"
      ],
      "metadata": {
        "id": "ZvOhfGHQf8rm"
      },
      "id": "ZvOhfGHQf8rm"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class EvaluationCheck(BaseModel):\n",
        "    check_name: str\n",
        "    justification: str\n",
        "    check_pass: bool\n",
        "\n",
        "class EvaluationChecklist(BaseModel):\n",
        "    checklist: list[EvaluationCheck]\n",
        "    summary: str"
      ],
      "metadata": {
        "id": "ZTjKBqHNf5uf"
      },
      "id": "ZTjKBqHNf5uf",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Pydantic AI in order to make the output follow the specified class, we use the parameter output_type:\n"
      ],
      "metadata": {
        "id": "kQEjARkSgOtR"
      },
      "id": "kQEjARkSgOtR"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_agent = Agent(\n",
        "    name='eval_agent',\n",
        "    model='gpt-5-nano',\n",
        "    instructions=evaluation_prompt,\n",
        "    output_type=EvaluationChecklist\n",
        ")\n"
      ],
      "metadata": {
        "id": "PISHs4tCgMRH"
      },
      "id": "PISHs4tCgMRH",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to run the agent, it needs input. We'll start with a template:\n"
      ],
      "metadata": {
        "id": "5wYgu6IAge1C"
      },
      "id": "5wYgu6IAge1C"
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt_format = \"\"\"\n",
        "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
        "<QUESTION>{question}</QUESTION>\n",
        "<ANSWER>{answer}</ANSWER>\n",
        "<LOG>{log}</LOG>\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "R_O_LZc4gbiG"
      },
      "id": "R_O_LZc4gbiG",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_log_file(log_file):\n",
        "    with open(log_file, 'r') as f_in:\n",
        "        log_data = json.load(f_in)\n",
        "        log_data['log_file'] = log_file\n",
        "        return log_data\n"
      ],
      "metadata": {
        "id": "OH0c17BIhnGw"
      },
      "id": "OH0c17BIhnGw",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
        "\n",
        "instructions = log_record['system_prompt']\n",
        "question = log_record['messages'][0]['parts'][0]['content']\n",
        "answer = log_record['messages'][-1]['parts'][0]['content']\n",
        "log = json.dumps(log_record['messages'])\n",
        "\n",
        "user_prompt = user_prompt_format.format(\n",
        "    instructions=instructions,\n",
        "    question=question,\n",
        "    answer=answer,\n",
        "    log=log\n",
        ")\n"
      ],
      "metadata": {
        "id": "A4Aa1O3wjCql"
      },
      "id": "A4Aa1O3wjCql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user input is ready and we can test it!"
      ],
      "metadata": {
        "id": "pUHJ_CSnjIu-"
      },
      "id": "pUHJ_CSnjIu-"
    },
    {
      "cell_type": "code",
      "source": [
        "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "\n",
        "checklist = result.output\n",
        "print(checklist.summary)\n",
        "\n",
        "for check in checklist.checklist:\n",
        "    print(check)"
      ],
      "metadata": {
        "id": "jv6fJh8_jGnG"
      },
      "id": "jv6fJh8_jGnG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we're putting the entire conversation log into the prompt, which is not really necessary. We can reduce it to make it less verbose.\n"
      ],
      "metadata": {
        "id": "mEyS1U09jjZx"
      },
      "id": "mEyS1U09jjZx"
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_log_messages(messages):\n",
        "    log_simplified = []\n",
        "\n",
        "    for m in messages:\n",
        "        parts = []\n",
        "\n",
        "        for original_part in m['parts']:\n",
        "            part = original_part.copy()\n",
        "            kind = part['part_kind']\n",
        "\n",
        "            if kind == 'user-prompt':\n",
        "                del part['timestamp']\n",
        "            if kind == 'tool-call':\n",
        "                del part['tool_call_id']\n",
        "            if kind == 'tool-return':\n",
        "                del part['tool_call_id']\n",
        "                del part['metadata']\n",
        "                del part['timestamp']\n",
        "                # Replace actual search results with placeholder to save tokens\n",
        "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
        "            if kind == 'text':\n",
        "                del part['id']\n",
        "\n",
        "            parts.append(part)\n",
        "\n",
        "        message = {\n",
        "            'kind': m['kind'],\n",
        "            'parts': parts\n",
        "        }\n",
        "\n",
        "        log_simplified.append(message)\n",
        "    return log_simplified\n"
      ],
      "metadata": {
        "id": "-QDzlvFRjgia"
      },
      "id": "-QDzlvFRjgia",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_log_record(eval_agent, log_record):\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    instructions = log_record['system_prompt']\n",
        "    question = messages[0]['parts'][0]['content']\n",
        "    answer = messages[-1]['parts'][0]['content']\n",
        "\n",
        "    log_simplified = simplify_log_messages(messages)\n",
        "    log = json.dumps(log_simplified)\n",
        "\n",
        "    user_prompt = user_prompt_format.format(\n",
        "        instructions=instructions,\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        log=log\n",
        "    )\n",
        "\n",
        "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "    return result.output\n",
        "\n",
        "\n",
        "log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
        "eval1 = await evaluate_log_record(eval_agent, log_record)"
      ],
      "metadata": {
        "id": "KrZ1zSrWjqZo"
      },
      "id": "KrZ1zSrWjqZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generation\n",
        "We can ask AI to help. What if we used it for generating more questions? Let's do that.\n",
        "We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record. We use this question as input to our agent and log the answers.\n",
        "Let’s start by defining the question generator:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T2O27TjtjwiZ"
      },
      "id": "T2O27TjtjwiZ"
    },
    {
      "cell_type": "code",
      "source": [
        "question_generation_prompt = \"\"\"\n",
        "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
        "\n",
        "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
        "\n",
        "The questions should:\n",
        "\n",
        "- Be natural and varied in style\n",
        "- Range from simple to complex\n",
        "- Include both specific technical questions and general course questions\n",
        "\n",
        "Generate one question for each record.\n",
        "\"\"\".strip()\n",
        "\n",
        "class QuestionsList(BaseModel):\n",
        "    questions: list[str]\n",
        "\n",
        "question_generator = Agent(\n",
        "    name=\"question_generator\",\n",
        "    instructions=question_generation_prompt,\n",
        "    model='gpt-4o-mini',\n",
        "    output_type=QuestionsList\n",
        ")\n"
      ],
      "metadata": {
        "id": "2Tra8ZgNnUAn"
      },
      "id": "2Tra8ZgNnUAn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we simply iterate over each of the question, ask our agent and log the results:\n"
      ],
      "metadata": {
        "id": "wSsVPVvuoEXa"
      },
      "id": "wSsVPVvuoEXa"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample = random.sample(fin_faq, 10)\n",
        "prompt_docs = [d['content'] for d in sample]\n",
        "prompt = json.dumps(prompt_docs)\n",
        "\n",
        "result = await question_generator.run(prompt)\n",
        "questions = result.output.questions"
      ],
      "metadata": {
        "id": "HETLo8e5nyol"
      },
      "id": "HETLo8e5nyol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, collect all the AI-generated logs for the v2 agent:"
      ],
      "metadata": {
        "id": "w7vtMEZNoSP_"
      },
      "id": "w7vtMEZNoSP_"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set = []\n",
        "\n",
        "for log_file in LOG_DIR.glob('*.json'):\n",
        "    if 'faq_agent_v2' not in log_file.name:\n",
        "        continue\n",
        "\n",
        "    log_record = load_log_file(log_file)\n",
        "    if log_record['source'] != 'ai-generated':\n",
        "        continue\n",
        "\n",
        "    eval_set.append(log_record)"
      ],
      "metadata": {
        "id": "H2B5n6cZoPs9"
      },
      "id": "H2B5n6cZoPs9",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And evaluate them:"
      ],
      "metadata": {
        "id": "a8DbUbPWoXoy"
      },
      "id": "a8DbUbPWoXoy"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = []\n",
        "\n",
        "for log_record in tqdm(eval_set):\n",
        "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
        "    eval_results.append((log_record, eval_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ffd87085bbd440db9e447f74be90599f",
            "0329cfd3d6024b75a5719722e5fa0ffd",
            "5bb2330d868a416ebd244094c14b94c3",
            "33ab75cb80624ca7990461ce3b705984",
            "f7dc15ff2fd24e23a7f9b878df6819cb",
            "bc065848fb5640888090a10ddd07d039",
            "9c89699de134478f992be8a6c7c33ac4",
            "00d59f07d42a46bbbbb7b6578479e806",
            "14f9f43abb344eadaf04b5e29e3dce09",
            "2eb41c7a6ddc413f867a519492d5a358",
            "6d85e2e1ec78438a8d51572b308c1847"
          ]
        },
        "id": "1VHPv60foVQr",
        "outputId": "c3e53c48-d76d-49c8-e8fb-3a1c92fed211"
      },
      "id": "1VHPv60foVQr",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd87085bbd440db9e447f74be90599f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code:\n",
        "Loops through each AI-generated log\n",
        "Runs our evaluation agent on it\n",
        "Stores both the original log and evaluation result\n"
      ],
      "metadata": {
        "id": "0H8jze5kocoA"
      },
      "id": "0H8jze5kocoA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are collected, but we need to display them and also calculate some statistics. The best tool for doing this is Pandas. We already should have it because minsearch depends on it.\n"
      ],
      "metadata": {
        "id": "XKiXIURqolJV"
      },
      "id": "XKiXIURqolJV"
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for log_record, eval_result in eval_results:\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    row = {\n",
        "        'file': log_record['log_file'].name,\n",
        "        'question': messages[0]['parts'][0]['content'],\n",
        "        'answer': messages[-1]['parts'][0]['content'],\n",
        "    }\n",
        "\n",
        "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
        "    row.update(checks)\n",
        "\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "aJIYh_lsopMu"
      },
      "id": "aJIYh_lsopMu",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code:\n",
        "Extracts key information from each log (file, question, answer)\n",
        "Converts the evaluation checks into a dictionary format\n"
      ],
      "metadata": {
        "id": "exWpJHZ_ouxh"
      },
      "id": "exWpJHZ_ouxh"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_evals = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "hZ8Ul3Ctoz9O"
      },
      "id": "hZ8Ul3Ctoz9O",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_evals.mean(numeric_only=True)"
      ],
      "metadata": {
        "id": "qN-IRfg-o7hm"
      },
      "id": "qN-IRfg-o7hm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "instructions_follow    0.3\n",
        "instructions_avoid     1.0\n",
        "answer_relevant        1.0\n",
        "answer_clear           1.0\n",
        "answer_citations       0.3\n",
        "completeness           0.7\n",
        "tool_call_search       1.0\n"
      ],
      "metadata": {
        "id": "-ht5lAeHo-l_"
      },
      "id": "-ht5lAeHo-l_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells us:\n",
        "Only 30% of responses follow instructions completely\n",
        "All responses avoid forbidden actions (good!)\n",
        "All responses are relevant and clear (great!)\n",
        "Only 30% include proper citations (needs improvement)\n",
        "70% of responses are complete\n",
        "All responses use the search tool (as expected)\n"
      ],
      "metadata": {
        "id": "aRZAp7Q1o_1L"
      },
      "id": "aRZAp7Q1o_1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating functions and tools\n"
      ],
      "metadata": {
        "id": "A_iAtl9oqhzu"
      },
      "id": "A_iAtl9oqhzu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can implement hitrate and MRR calculation in Python:"
      ],
      "metadata": {
        "id": "sIXCUkKZ6Tac"
      },
      "id": "sIXCUkKZ6Tac"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_search_quality(search_function, test_queries):\n",
        "    results = []\n",
        "\n",
        "    for query, expected_docs in test_queries:\n",
        "        search_results = search_function(query, num_results=5)\n",
        "\n",
        "        # Calculate hit rate\n",
        "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
        "\n",
        "        # Calculate MRR\n",
        "        for i, doc in enumerate(search_results):\n",
        "            if doc['filename'] in expected_docs:\n",
        "                mrr = 1 / (i + 1)\n",
        "                break\n",
        "        else:\n",
        "            mrr = 0\n",
        "\n",
        "        results.append({\n",
        "            'query': query,\n",
        "            'hit': relevant_found,\n",
        "            'mrr': mrr\n",
        "        })\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Xqw19DuNq_9a"
      },
      "id": "Xqw19DuNq_9a",
      "execution_count": 64,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c37d5d241d754c4fa0ae5fe9d77f9735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3fe8ef401a94a13bb8894b5f33fa92a",
              "IPY_MODEL_b9ca580cfa274544acf829d85fa24b42",
              "IPY_MODEL_a3514fcebbff4dae9731157a6de4891a"
            ],
            "layout": "IPY_MODEL_70e5301f9f32420eb1d5c5e5b45479f5"
          }
        },
        "e3fe8ef401a94a13bb8894b5f33fa92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50474af7dcc2438a9f48d4fbefb35266",
            "placeholder": "​",
            "style": "IPY_MODEL_81df698bba3b4d338a651b3defcd93ee",
            "value": "100%"
          }
        },
        "b9ca580cfa274544acf829d85fa24b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7b512fd7e5743dda634292ea8721e70",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f88d5d99c22d43c2a4561b2492535e91",
            "value": 2
          }
        },
        "a3514fcebbff4dae9731157a6de4891a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a61fedfc98457fbeda697dd2b4dbc2",
            "placeholder": "​",
            "style": "IPY_MODEL_6971d33e9e8c4542b79b0ad93789e501",
            "value": " 2/2 [00:00&lt;00:00, 192.16it/s]"
          }
        },
        "70e5301f9f32420eb1d5c5e5b45479f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50474af7dcc2438a9f48d4fbefb35266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81df698bba3b4d338a651b3defcd93ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7b512fd7e5743dda634292ea8721e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88d5d99c22d43c2a4561b2492535e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78a61fedfc98457fbeda697dd2b4dbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6971d33e9e8c4542b79b0ad93789e501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffd87085bbd440db9e447f74be90599f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0329cfd3d6024b75a5719722e5fa0ffd",
              "IPY_MODEL_5bb2330d868a416ebd244094c14b94c3",
              "IPY_MODEL_33ab75cb80624ca7990461ce3b705984"
            ],
            "layout": "IPY_MODEL_f7dc15ff2fd24e23a7f9b878df6819cb"
          }
        },
        "0329cfd3d6024b75a5719722e5fa0ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc065848fb5640888090a10ddd07d039",
            "placeholder": "​",
            "style": "IPY_MODEL_9c89699de134478f992be8a6c7c33ac4",
            "value": ""
          }
        },
        "5bb2330d868a416ebd244094c14b94c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d59f07d42a46bbbbb7b6578479e806",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14f9f43abb344eadaf04b5e29e3dce09",
            "value": 0
          }
        },
        "33ab75cb80624ca7990461ce3b705984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eb41c7a6ddc413f867a519492d5a358",
            "placeholder": "​",
            "style": "IPY_MODEL_6d85e2e1ec78438a8d51572b308c1847",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f7dc15ff2fd24e23a7f9b878df6819cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc065848fb5640888090a10ddd07d039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c89699de134478f992be8a6c7c33ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d59f07d42a46bbbbb7b6578479e806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "14f9f43abb344eadaf04b5e29e3dce09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2eb41c7a6ddc413f867a519492d5a358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d85e2e1ec78438a8d51572b308c1847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}